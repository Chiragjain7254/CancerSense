# -*- coding: utf-8 -*-
"""CancerSense.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1phaQp_Cd3X1sF199tJckojDG8YBIjqMF
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets
from sklearn.model_selection import train_test_split

# loading the data from sklearn
breast_cancer_dataset = sklearn.datasets.load_breast_cancer()

print(breast_cancer_dataset)

# loading the data to a data frame
data_frame = pd.DataFrame(breast_cancer_dataset.data, columns=breast_cancer_dataset.feature_names)

# print the first 5 rows of the dataframe
data_frame.head()

# adding the 'target' column to the data frame
data_frame['label'] = breast_cancer_dataset.target

# print last 5 rows of the dataframe
data_frame.tail()

# number of rows and columns in the dataset
data_frame.shape

# getting some information about the data
data_frame.info()

# checking for missing values
data_frame.isnull().sum()

# statistical measures about the data
data_frame.describe()

# checking the distribution of Target Variable
data_frame['label'].value_counts()

data_frame.groupby('label').mean()

# Separating the features and target
X = data_frame.drop(columns='label', axis=1)
Y = data_frame['label']

print(X)

print(Y)

# Splitting the data into training data & Testing data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

# Splitting the data into training data & Testing data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

# Standardize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_std = scaler.fit_transform(X_train)

X_test_std = scaler.transform(X_test)

# importing necessary libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# set random seed for reproducibility
tf.random.set_seed(3)

# setting up the layers of the neural network
model = Sequential([
    # Input layer: Flatten layer to convert input shape (30,) to a 1D array
    Dense(128, input_shape=(30,), activation='relu'),

    # Hidden layers: Adding more dense layers with relu activation
    Dense(256, activation='relu'),
    Dropout(0.5),  # Dropout layer to prevent overfitting by randomly dropping 50% of neurons

    Dense(128, activation='relu'),
    BatchNormalization(),  # Batch normalization to stabilize and accelerate the training process

    Dense(64, activation='relu'),
    Dropout(0.4),  # Dropout layer to prevent overfitting by randomly dropping 40% of neurons

    Dense(32, activation='relu'),
    BatchNormalization(),

    Dense(16, activation='relu'),
    Dropout(0.3),  # Dropout layer to prevent overfitting by randomly dropping 30% of neurons

    # Output layer: Dense layer with softmax activation for multi-class classification
    Dense(2, activation='softmax')
])



# Early stopping to prevent overfitting and save training time
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Print model summary
model.summary()

# Compile the model
model.compile(optimizer='adam',  # Using Adam optimizer for optimization
              loss='sparse_categorical_crossentropy',  # Sparse categorical crossentropy for multi-class classification
              metrics=['accuracy'])  # Monitoring accuracy during training

# Early stopping to prevent overfitting and save training time
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Print model summary
model.summary()

# training the Meural Network

history = model.fit(X_train_std, Y_train, validation_split=0.1, epochs=10)

"""Visualizing accuracy and loss"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'upper right')

"""Accuracy of the model on test data"""

loss, accuracy = model.evaluate(X_test_std, Y_test)
print(accuracy)

print(X_test_std.shape)
print(X_test_std[0])

Y_pred = model.predict(X_test_std)

print(Y_pred.shape)
print(Y_pred[0])

print(X_test_std)

print(Y_pred)

"""model.predict() gives the prediction probability of each class for that data point"""

#  argmax function

my_list = [0.25, 0.56]

index_of_max_value = np.argmax(my_list)
print(my_list)
print(index_of_max_value)

# converting the prediction probability to class labels

Y_pred_labels = [np.argmax(i) for i in Y_pred]
print(Y_pred_labels)

"""**Building the predictive system**"""

input_data = (11.76,21.6,74.72,427.9,0.08637,0.04966,0.01657,0.01115,0.1495,0.05888,0.4062,1.21,2.635,28.47,0.005857,0.009758,0.01168,0.007445,0.02406,0.001769,12.98,25.72,82.98,516.5,0.1085,0.08615,0.05523,0.03715,0.2433,0.06563)

# change the input_data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array as we are predicting for one data point
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardizing the input data
input_data_std = scaler.transform(input_data_reshaped)

prediction = model.predict(input_data_std)
print(prediction)

prediction_label = [np.argmax(prediction)]
print(prediction_label)

if(prediction_label[0] == 0):
  print('The tumor is Malignant')

else:
  print('The tumor is Benign')

# Plot histograms for a few features
features_to_plot = ['mean radius', 'mean texture', 'mean perimeter']

for feature in features_to_plot:
    plt.figure(figsize=(8, 6))
    plt.hist(data_frame[data_frame['label'] == 0][feature], bins=30, alpha=0.5, label='Malignant')
    plt.hist(data_frame[data_frame['label'] == 1][feature], bins=30, alpha=0.5, label='Benign')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.title(f'Histogram of {feature}')
    plt.legend()
    plt.show()